{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awaliaaaa23/Final-Project-AMS-2025/blob/main/Ambil_Data_Komentar_Youtube_dengan_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "import googleapiclient.discovery\n",
        "\n",
        "api_key = \"AIzaSyCK3f-B1wyE7QZJ-cyD-IIgB5Pfx-kCgm8\" # Replace with your actual API key\n",
        "\n",
        "keywords = ['#produkisrael', '#boikotprodukisrael', '#aksiboikot', '#penggantiprodukisrael', \"#boikotkfc\", '#boikotunilever', '#boikotstarbuck', '#dampakboikot']\n",
        "\n",
        "used_video_ids = set()  # Untuk menyimpan video yang sudah dipakai\n",
        "\n",
        "def search_and_get_comments(search_query, max_videos=10, max_comments_per_video=250, start_date=None, end_date=None, csv_filename=\"youtube_comments.csv\"):\n",
        "    youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "\n",
        "    search_response = youtube.search().list(\n",
        "        part=\"snippet\",\n",
        "        q=search_query,\n",
        "        type=\"video\",\n",
        "        maxResults=30  # ambil lebih banyak untuk jaga-jaga (nanti kita filter jadi 10 unik)\n",
        "    ).execute()\n",
        "\n",
        "    # Filter video yang belum dipakai\n",
        "    video_titles_and_ids = {}\n",
        "    for item in search_response['items']:\n",
        "        if item['id']['kind'] == 'youtube#video':\n",
        "            video_id = item['id']['videoId']\n",
        "            if video_id not in used_video_ids:\n",
        "                video_titles_and_ids[video_id] = item['snippet']['title']\n",
        "                if len(video_titles_and_ids) >= max_videos:\n",
        "                    break\n",
        "\n",
        "    used_video_ids.update(video_titles_and_ids.keys())  # Tandai video ini sudah terpakai\n",
        "\n",
        "    if not video_titles_and_ids:\n",
        "        print(f\"No new videos found for keyword '{search_query}'. Skipping.\")\n",
        "        return\n",
        "\n",
        "    total_comments = 0\n",
        "    unique_comments = set()\n",
        "\n",
        "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\") if start_date else None\n",
        "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\") if end_date else None\n",
        "\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"keyword\", \"video_id\", \"video_title\", \"comment_text\", \"published_at\"])  # Header\n",
        "\n",
        "        for video_id, video_title in video_titles_and_ids.items():\n",
        "            try:\n",
        "                next_page_token = None\n",
        "                comments_written = 0\n",
        "\n",
        "                while True:\n",
        "                    request = youtube.commentThreads().list(\n",
        "                        part=\"snippet\",\n",
        "                        videoId=video_id,\n",
        "                        maxResults=100,\n",
        "                        pageToken=next_page_token\n",
        "                    )\n",
        "                    response = request.execute()\n",
        "\n",
        "                    for item in response['items']:\n",
        "                        try:\n",
        "                            snippet = item['snippet']['topLevelComment']['snippet']\n",
        "                            comment_text = snippet['textDisplay']\n",
        "                            published_at = snippet['publishedAt']\n",
        "                            comment_dt = datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "                            if (start_dt and comment_dt < start_dt) or (end_dt and comment_dt > end_dt):\n",
        "                                continue\n",
        "\n",
        "                            unique_id = f\"{video_id}_{comment_text}\"\n",
        "                            if unique_id in unique_comments:\n",
        "                                continue\n",
        "                            unique_comments.add(unique_id)\n",
        "\n",
        "                            writer.writerow([search_query, video_id, video_title, comment_text, published_at])\n",
        "                            total_comments += 1\n",
        "                            comments_written += 1\n",
        "\n",
        "                            if comments_written >= max_comments_per_video:\n",
        "                                break\n",
        "                        except KeyError:\n",
        "                            continue\n",
        "\n",
        "                    next_page_token = response.get(\"nextPageToken\")\n",
        "                    if not next_page_token or comments_written >= max_comments_per_video:\n",
        "                        break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error retrieving comments for video ID {video_id}: {e}\")\n",
        "\n",
        "    print(f\"CSV file '{csv_filename}' created for keyword '{search_query}'.\")\n",
        "    print(f\"Total unique comments retrieved: {total_comments}\")\n",
        "    print(f\"Number of videos fetched: {len(video_titles_and_ids)}\")\n",
        "\n",
        "# Run for all keywords\n",
        "start_date = \"2023-10-7\"\n",
        "end_date = \"2025-07-15\"\n",
        "\n",
        "for keyword in keywords:\n",
        "    csv_file = f\"youtube_comments_{keyword.replace(' ', '_')}.csv\"\n",
        "    search_and_get_comments(keyword, max_videos=10, max_comments_per_video=250, start_date=start_date, end_date=end_date, csv_filename=csv_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcpLnLZQG3Mu",
        "outputId": "bfee83ac-0388-436c-f89f-d76971060dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'youtube_comments_#produkisrael.csv' created for keyword '#produkisrael'.\n",
            "Total unique comments retrieved: 1388\n",
            "Number of videos fetched: 10\n",
            "CSV file 'youtube_comments_#boikotprodukisrael.csv' created for keyword '#boikotprodukisrael'.\n",
            "Total unique comments retrieved: 1349\n",
            "Number of videos fetched: 10\n",
            "CSV file 'youtube_comments_#aksiboikot.csv' created for keyword '#aksiboikot'.\n",
            "Total unique comments retrieved: 69\n",
            "Number of videos fetched: 10\n",
            "CSV file 'youtube_comments_#penggantiprodukisrael.csv' created for keyword '#penggantiprodukisrael'.\n",
            "Total unique comments retrieved: 692\n",
            "Number of videos fetched: 10\n",
            "CSV file 'youtube_comments_#boikotkfc.csv' created for keyword '#boikotkfc'.\n",
            "Total unique comments retrieved: 1202\n",
            "Number of videos fetched: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error retrieving comments for video ID P2DfP8u4DgQ: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=P2DfP8u4DgQ&maxResults=100&key=AIzaSyCK3f-B1wyE7QZJ-cyD-IIgB5Pfx-kCgm8&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
            "CSV file 'youtube_comments_#boikotunilever.csv' created for keyword '#boikotunilever'.\n",
            "Total unique comments retrieved: 400\n",
            "Number of videos fetched: 10\n",
            "CSV file 'youtube_comments_#boikotstarbuck.csv' created for keyword '#boikotstarbuck'.\n",
            "Total unique comments retrieved: 1044\n",
            "Number of videos fetched: 10\n",
            "CSV file 'youtube_comments_#dampakboikot.csv' created for keyword '#dampakboikot'.\n",
            "Total unique comments retrieved: 932\n",
            "Number of videos fetched: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import glob\n",
        "\n",
        "# Simpan semua komentar yang ditemukan\n",
        "all_comments = {}\n",
        "\n",
        "# Baca semua file CSV hasil scraping\n",
        "for file_name in glob.glob(\"youtube_comments_*.csv\"):\n",
        "    with open(file_name, mode='r', encoding='utf-8') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            comment_id = f\"{row['video_id']}_{row['comment_text'].strip()}\"\n",
        "            if comment_id not in all_comments:\n",
        "                all_comments[comment_id] = {\n",
        "                    'count': 1,\n",
        "                    'details': [(file_name, row['video_id'], row['video_title'], row['comment_text'], row['published_at'])]\n",
        "                }\n",
        "            else:\n",
        "                all_comments[comment_id]['count'] += 1\n",
        "                all_comments[comment_id]['details'].append((file_name, row['video_id'], row['video_title'], row['comment_text'], row['published_at']))\n",
        "\n",
        "# Filter komentar yang muncul lebih dari sekali (duplikat antar file)\n",
        "duplicates = {k: v for k, v in all_comments.items() if v['count'] > 1}\n",
        "\n",
        "# Tulis laporan duplikat ke file CSV baru\n",
        "with open('duplicate_comments_report.csv', mode='w', newline='', encoding='utf-8') as report_file:\n",
        "    writer = csv.writer(report_file)\n",
        "    writer.writerow(['video_id', 'comment_text', 'file_1', 'file_2', 'published_at'])\n",
        "\n",
        "    for comment_id, data in duplicates.items():\n",
        "        # Ambil dua contoh file pertama tempat duplikat muncul\n",
        "        details = data['details']\n",
        "        writer.writerow([\n",
        "            details[0][1],  # video_id\n",
        "            details[0][3],  # comment_text\n",
        "            details[0][0],  # file_1\n",
        "            details[1][0],  # file_2\n",
        "            details[0][4]   # published_at\n",
        "        ])\n",
        "\n",
        "print(f\"Duplicate check finished. Found {len(duplicates)} duplicate comments across files.\")\n",
        "print(\"Report saved as 'duplicate_comments_report.csv'.\")\n"
      ],
      "metadata": {
        "id": "Nh4tjN-jIxMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb9e0541-cf69-45eb-ea8d-41c659744f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate check finished. Found 0 duplicate comments across files.\n",
            "Report saved as 'duplicate_comments_report.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import glob\n",
        "\n",
        "# Siapkan file gabungan\n",
        "output_file = \"all_youtube_comments_combined.csv\"\n",
        "\n",
        "# Cari semua file yang cocok\n",
        "csv_files = glob.glob(\"youtube_comments_*.csv\")\n",
        "\n",
        "if not csv_files:\n",
        "    print(\"No youtube_comments_*.csv files found.\")\n",
        "else:\n",
        "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        writer = None\n",
        "\n",
        "        for i, file_name in enumerate(csv_files):\n",
        "            with open(file_name, mode='r', encoding='utf-8') as infile:\n",
        "                reader = csv.reader(infile)\n",
        "                headers = next(reader)  # ambil header\n",
        "\n",
        "                # Tulis header sekali saja\n",
        "                if writer is None:\n",
        "                    writer = csv.writer(outfile)\n",
        "                    writer.writerow(headers)\n",
        "\n",
        "                for row in reader:\n",
        "                    writer.writerow(row)\n",
        "\n",
        "    print(f\"All CSV files combined into '{output_file}'. Total files merged: {len(csv_files)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAHecOz1kABJ",
        "outputId": "8a2f0b70-aca5-4757-84c1-13fd548634f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All CSV files combined into 'all_youtube_comments_combined.csv'. Total files merged: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"all_youtube_comments_combined.csv\")\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOTezkYJkEY0",
        "outputId": "aa1eb022-9e97-439f-f5be-938562602d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7076, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMIS0MpVkSJg",
        "outputId": "cea916f5-e2f9-4d08-e87d-cd619b73cdd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['keyword', 'video_id', 'video_title', 'comment_text', 'published_at'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "gFbtF0cslOjl",
        "outputId": "6353d936-01ac-404a-a102-51c5d12b0de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keyword         0\n",
              "video_id        0\n",
              "video_title     0\n",
              "comment_text    0\n",
              "published_at    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>keyword</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>video_id</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>video_title</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comment_text</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>published_at</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}